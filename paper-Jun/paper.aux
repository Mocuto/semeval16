\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{o2010tweets}
\citation{bollen2011twitter}
\citation{bollen2009modeling}
\citation{mehrotra2013improving}
\citation{kalchbrenner2014convolutional}
\citation{severynunitn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kouloumpis2011twitter}
\citation{wang2011topic}
\citation{mikolov2013efficient}
\citation{kim2014convolutional}
\citation{mikolov2013distributed}
\citation{tang2014learning}
\citation{kalchbrenner2014convolutional}
\citation{zhang2015character}
\citation{hochreiter1997long}
\citation{li2015tree}
\citation{kalchbrenner2014convolutional}
\citation{severynunitn}
\citation{mikolov2013distributed}
\citation{kalchbrenner2014convolutional}
\citation{kalchbrenner2014convolutional}
\citation{kim2014convolutional}
\citation{severynunitn}
\citation{kalchbrenner2014convolutional}
\citation{kalchbrenner2014convolutional}
\citation{graves2012supervised}
\citation{karpathy-rnn}
\citation{hochreiter2001gradient}
\citation{hochreiter1997long}
\citation{colah}
\citation{graves2012supervised}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dynamic Convolutional Neural Networks (DCNN)}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Recurrent Neural Networks with Long Short Term Memory (LSTM)}{2}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Dynamic Convulational Neural Network of \cite  {kalchbrenner2014convolutional} (Source: \cite  {kalchbrenner2014convolutional})}}{3}{figure.1}}
\newlabel{fig:dcnn}{{1}{3}{Dynamic Convulational Neural Network of \cite {kalchbrenner2014convolutional} (Source: \cite {kalchbrenner2014convolutional})}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNNs allow modeling of multiple types of input and output sequences (Source: \cite  {karpathy-rnn})}}{3}{figure.2}}
\newlabel{fig:rnn}{{2}{3}{RNNs allow modeling of multiple types of input and output sequences (Source: \cite {karpathy-rnn})}{figure.2}{}}
\citation{colah}
\citation{colah}
\citation{kalchbrenner2014convolutional}
\citation{mikolov2013distributed}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The repeating module of LSTM. $x_t$ is the input as time $t$ and $h_t$ is the output from the LSTM output gate at time $t$. The top horizontal line corresponds to the cell state and the bottom line corresponds to the hidden state (both of which are recurring states). (Source: \cite  {colah})}}{4}{figure.3}}
\newlabel{fig:lstm}{{3}{4}{The repeating module of LSTM. $x_t$ is the input as time $t$ and $h_t$ is the output from the LSTM output gate at time $t$. The top horizontal line corresponds to the cell state and the bottom line corresponds to the hidden state (both of which are recurring states). (Source: \cite {colah})}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Twitter Sentiment Analysis using Character LSTM}{4}{section.3}}
\bibdata{paper}
\bibcite{bollen2011twitter}{1}
\bibcite{bollen2009modeling}{2}
\bibcite{go2009twitter}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The bi-directional LSTM model used for Twitter sentiment analysis. Each circular node represents an LSTM cell. The input to the model is the sequence $x$. }}{5}{figure.4}}
\newlabel{fig:mylstm}{{4}{5}{The bi-directional LSTM model used for Twitter sentiment analysis. Each circular node represents an LSTM cell. The input to the model is the sequence $x$}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sample table title}}{5}{table.1}}
\newlabel{sample-table}{{1}{5}{Sample table title}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{section.5}}
\bibcite{graves2012supervised}{4}
\bibcite{hochreiter2001gradient}{5}
\bibcite{hochreiter1997long}{6}
\bibcite{kalchbrenner2014convolutional}{7}
\bibcite{karpathy-rnn}{8}
\bibcite{kim2014convolutional}{9}
\bibcite{kouloumpis2011twitter}{10}
\bibcite{li2015tree}{11}
\bibcite{mehrotra2013improving}{12}
\bibcite{mikolov2013efficient}{13}
\bibcite{mikolov2013distributed}{14}
\bibcite{o2010tweets}{15}
\bibcite{colah}{16}
\bibcite{severynunitn}{17}
\bibcite{tang2014learning}{18}
\bibcite{wang2011topic}{19}
\bibcite{zhang2015character}{20}
\bibstyle{plain}
